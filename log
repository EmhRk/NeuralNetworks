project start on 2019-10-2
better finish before the end of October holiday

(input)-[active function, weight]->(hiddenLayer)-[active function, weight]->(output)
        [       IH(i, j)        ]                [        HO(i, j)       ]

parameter:
errorRate=0.1, maxIter=200, learnRateIH=0.2, learnRateHO=0.2
result:
0.1    1  times
0.033  13 times
0.433  2  times
0.066  1  times
0.366  2  times     in 20 times(total)

2019-10-31
为什么才两层grad就消失了
我裂开了
检查ReLU层，感觉不太对劲。。。

2019-12-3
过了一个月我又出现了哈哈哈哈
因为ddl来了
更换输出为softmax函数
损失函数为交叉熵函数
内层激活函数还原为sigmoid函数
希望有用